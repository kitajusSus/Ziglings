\documentclass[12pt,a4paper]{article}

% -- USTAWIENIA PODSTAWOWE --
\usepackage[utf8]{inputenc}          % Kodowanie znaków
\usepackage[T1]{fontenc}             % Kodowanie fontów (polskie znaki)
\usepackage{lmodern}                 % Lepsze fonty wektorowe
\usepackage[polish]{babel}           % Język polski (dzielenie wyrazów, itp.)
\usepackage{amsmath,amsfonts,amssymb} % Dodatkowe symbole matematyczne
\usepackage{graphicx}                % Wstawianie grafiki
\usepackage{geometry}                % Ustawienie marginesów
\usepackage{hyperref}                % Hiperłącza w dokumencie
\usepackage{xcolor}                  % Kolory
\usepackage{listings}                % Wstawki z kodem źródłowym
\usepackage{caption}                 % Legendy (caption) w floatach
\usepackage{subcaption}              % Podobne do wyżej - subfigures
\usepackage{float}                   % Kontrola pozycji floatów
\usepackage{natbib}                  % Styl cytowań/bibliografia
\usepackage{url}                     % Lepsza obsługa linków

% -- Ustawienie marginesów --
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=30mm,
}

% -- Ustawienia wyglądu kodu (listings) --
\lstset{
    basicstyle=\small\ttfamily,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    showstringspaces=false,
    frame=single,
    tabsize=4
}

% Tytuł, autor i data
\title{\textbf{Meta-learning i automatyczne dostosowywanie hiperparametrów w uczeniu maszynowym}\\
\large{Przygotowanie teoretyczne, przykłady w Pythonie i Zig}}
\author{Imię i Nazwisko \\
\small{Uczelnia / Instytucja}\\
\small{\texttt{adres\_email@example.com}}}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section*{Wstęp}
\addcontentsline{toc}{section}{Wstęp}

Wraz z rozwojem uczenia maszynowego i głębokich sieci neuronowych, nastąpił wzrost zapotrzebowania na metody automatycznej optymalizacji \textbf{hiperparametrów} i struktury modeli. W praktyce, wiele udanych aplikacji uczenia maszynowego (np. rozpoznawanie mowy, przetwarzanie języka naturalnego, robotyka) wymaga dostrajania (ang. \emph{fine-tuning}) setek lub tysięcy hiperparametrów, co jest bardzo pracochłonne i kosztowne obliczeniowo. 

\emph{Meta-learning} (zwany też \emph{uczeniem na metapoziomie} czy \emph{uczeniem się jak się uczyć}) stanowi jeden z najważniejszych kierunków, pozwalając na \emph{automatyczne} bądź półautomatyczne odkrywanie optymalnych wartości hiperparametrów, schematów optymalizacji, a nawet struktur sieci. Celem tego dokumentu jest przedstawienie \textbf{kompleksowego} omówienia (1) teoretycznych podstaw meta-learningu i jego znaczenia w optymalizacji hiperparametrów, (2) wybranych algorytmów meta-learningu (np. MAML) i metod opartych na RL, (3) przykładów praktycznych w językach Python i Zig, oraz (4) kluczowych aspektów implementacji HPC (High Performance Computing) w Zig.

\newpage
\section{Podstawy uczenia maszynowego i hiperparametry}

\subsection{Klasyczne uczenie nadzorowane}

W uczeniu nadzorowanym zazwyczaj stawiamy sobie zadanie minimalizacji funkcji kosztu:
\begin{equation}
    L(\theta) = \frac{1}{N}\sum_{i=1}^{N} \ell(f_\theta(\mathbf{x}_i), y_i),
\end{equation}
gdzie:
\begin{itemize}
    \item \(\theta\) — wektor wag modelu (parametry),
    \item \(\mathbf{x}_i\) — wejście (np. wektor cech lub obraz),
    \item \(y_i\) — oczekiwany wynik (np. etykieta w klasyfikacji),
    \item \(\ell(\cdot)\) — funkcja błędu (np. błąd średniokwadratowy, entropia krzyżowa),
    \item \(N\) — liczba przykładów w zbiorze treningowym.
\end{itemize}

Optymalizacja najczęściej odbywa się poprzez \emph{spadek wzdłuż gradientu} (ang. \emph{gradient descent}) lub jego warianty (Adam, RMSProp, itp.). 

\subsection{Hiperparametry}
\label{sec:hyperparams}

\textbf{Hiperparametry} są to te parametry, które nie są bezpośrednio wyuczalne w trakcie podstawowego procesu treningu, ale \emph{muszą} być ustalone przed rozpoczęciem lub w trakcie na wyższym poziomie (np. \emph{learning rate}, \emph{batch size}, współczynniki regularizacji). Wpływają one często znacząco na szybkość konwergencji i końcową jakość modelu.

\textbf{Przykłady hiperparametrów}:
\begin{enumerate}
    \item \textit{Learning rate} (\(\eta\)).
    \item \textit{Momentum} czy \(\beta_1, \beta_2\) w Adam/RMSProp.
    \item \textit{Liczba warstw} i \textit{liczba neuronów} w sieci (architektura).
    \item \textit{Współczynnik regularizacji} (np. \(\lambda\)).
    \item \textit{Funkcje aktywacji} (ReLU, tanh, itp.).
\end{enumerate}

\section{Meta-learning: definicja i podstawowe idee}

\subsection{Pojęcie meta-learningu}

W literaturze \citep{hochreiter2001learning,finn2017maml,ravi2016optimization} \emph{meta-learning} bywa definiowane jako proces, w którym algorytm (nazywany \emph{meta-uczniem}) zdobywa wiedzę o tym, \textbf{jak} szkolić modele (zwane \emph{uczniami bazowymi}) na różnych zadaniach. Innymi słowy, meta-uczeń (ang. \emph{meta-learner}) uczy się:
\begin{itemize}
    \item Wybierać lub dostrajać \emph{hiperparametry} modelu bazowego.
    \item Projektować kroki optymalizacji (np. uczenie się własnego optymalizatora).
    \item Szybko adaptować się do nowych zadań (tzw. \emph{few-shot learning}).
\end{itemize}

\subsection{Rodzaje meta-learningu}

Meta-learning można podzielić na różne kategorie, m.in.:
\begin{enumerate}
    \item \textbf{Bazujące na optymalizacji}: przykładem jest \emph{Model-Agnostic Meta-Learning (MAML)} \citep{finn2017maml}, w którym meta-uczeń \emph{modyfikuje} wagi tak, aby były optymalne w sensie szybkiej adaptacji do nowych zadań.
    \item \textbf{Uczenie się własnych hiperparametrów/algorytmów}: agent RL lub sieć neuronowa przewiduje wartości \emph{learning rate} w kolejnych krokach.
    \item \textbf{Probabilistyczne} (Bayesowskie): gdzie hiperparametry traktujemy jako rozkłady, a meta-learning polega na aktualizacji posterioru dla kolejnych zadań.
\end{enumerate}

\subsection{Przykład: Model-Agnostic Meta-Learning (MAML)}

\emph{MAML} \citep{finn2017maml} w skrócie:
\begin{enumerate}
    \item Mamy zbiór zadań \(\{ T_1, T_2, \dots, T_k \}\).
    \item Każde zadanie to osobny zbiór danych (\emph{train} i \emph{validation/test}).
    \item \textbf{Meta-parametry} \(\theta\) to inicjalizacja wag dla modelu bazowego.
    \item Dla zadania \(T_i\):
    \begin{enumerate}
        \item Tworzymy kopię \(\theta'\) i wykonujemy kilka kroków uczenia na train (lokalne dostrojenie).
        \item Obliczamy stratę \(\mathcal{L}_i(\theta')\) na zbiorze walidacyjnym \emph{danego} zadania.
    \end{enumerate}
    \item \textbf{Meta-aktualizacja}: 
    \[
        \theta \leftarrow \theta - \alpha \nabla_\theta \sum_i \mathcal{L}_i\bigl(\theta'_i(\theta)\bigr),
    \]
    gdzie \(\theta'_i(\theta)\) oznacza wagi po lokalnych krokach dla zadania \(T_i\).
\end{enumerate}
W efekcie, \(\theta\) jest taką „uniwersalną” inicjalizacją, z której łatwiej i szybciej wyuczyć się (fine-tuning) dla nowych zadań.

\section{Automatyzacja doboru hiperparametrów za pomocą meta-learningu}

\subsection{Ogólne podejście}

Jednym z głównych zastosowań meta-learningu jest \textbf{automatyczne dostrajanie} hiperparametrów. Wyobraźmy sobie, że mamy algorytm, który w trakcie treningu:
\begin{itemize}
    \item Obserwuje bieżący koszt/loss.
    \item Aktualizuje \(\eta\) (learning rate) \textit{w locie} na podstawie meta-reguły.
    \item Decyduje, czy zmienić optymalizator (np. z Adam na RMSProp), jeśli pewne wskaźniki jakości spadają.
\end{itemize}

\textbf{Korzyść} polega na tym, że nie musimy wykonywać żmudnych eksperymentów typu \emph{grid search} czy \emph{random search} w przestrzeni hiperparametrów.

\subsection{Przykład z RL (Q-Learning)}

Można potraktować dobór hiperparametrów jako problem \emph{reinforcement learning}, gdzie:
\begin{itemize}
    \item \textbf{Stan} (ang. \emph{state}) = stan treningu: np. wskaźnik błędu, epoka, dotychczasowy learning rate, itp.
    \item \textbf{Akcja} (ang. \emph{action}) = wybór wartości hiperparametru (np. \(\eta\)) lub wybór optymalizatora.
    \item \textbf{Reward} = odwrotność błędu (np. ujemna strata) lub przyrost jakości w walidacji.
\end{itemize}

\subsection{Zastosowanie w dużych modelach (np. Whisper, GPT)}

W ostatnim czasie popularne jest \emph{fine-tuning} dużych modeli językowych (LLM) lub systemów STT (Whisper) przy użyciu metod niskorangowej adaptacji (LoRA, QLoRA). Dobór hiperparametrów (jak np. \emph{learning rate} głównych wag vs. wag adaptacyjnych) może być trudny. Tutaj meta-learning też znajduje zastosowanie, aby:
\begin{itemize}
    \item Szybko sprawdzić, jaka kombinacja \(\eta_{\text{LoRA}}\) i \(\eta_{\text{main}}\) minimalizuje błąd transkrypcji (WER).
    \item Dynamicznie przełączać się między trybami treningu (np. freeze/unfreeze warstw).
\end{itemize}

\section{Implementacje praktyczne i HPC w Zig}

\subsection{Dlaczego Zig?}

\textbf{Zig} jest językiem systemowym zbliżonym do C/C++, ale oferującym:
\begin{itemize}
    \item Prostszy model zarządzania pamięcią.
    \item Silne wsparcie dla \emph{compile-time} generowania kodu.
    \item Możliwość łatwego wykorzystania SIMD (SSE, AVX) i API GPU (CUDA/Vulkan).
    \item Wysoką kontrolę nad layoutem pamięci (np. pod tensory).
\end{itemize}
Jest to atrakcyjna alternatywa w miejscach, gdzie Python (i biblioteki takie jak PyTorch) są niewystarczająco wydajne dla konkretnych fragmentów HPC. Możemy np. \emph{zaimplementować} krytyczną funkcję (macierzowe mnożenie, kroki meta-learningu) w Zig, a wywoływać ją z poziomu Pythona (FFI).

\subsection{Przykłady w Pythonie i Zig}

Poniżej przedstawiamy obszerne przykłady kodu.

\subsubsection{Przykład w Pythonie: dynamiczna adaptacja learning rate}

\begin{lstlisting}[language=Python, caption=Adaptacja learning rate w trakcie treningu (PyTorch + pseudo meta-learning)]
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleMLP(nn.Module):
    def __init__(self, in_dim=2, hidden_dim=4, out_dim=1):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, out_dim)
    
    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

def dynamic_lr_training():
    # Przygotowanie danych (sztuczne)
    X = torch.randn(100, 2)
    y = (X[:,0] + X[:,1] > 0).float().unsqueeze(-1)

    model = SimpleMLP()
    criterion = nn.BCEWithLogitsLoss()

    # Inicjalna wartosc LR
    init_lr = 0.01
    optimizer = optim.SGD(model.parameters(), lr=init_lr)
    
    # "meta-parametr" - wspolczynnik skalujący LR
    meta_alpha = 1.0

    for epoch in range(20):
        # Krok forward
        logits = model(X)
        loss = criterion(logits, y)

        optimizer.zero_grad()
        loss.backward()

        # Oblicz "meta feedback"
        # np. jesli loss jest wiekszy niz poprzednio,
        # to zwieksz learning rate, w przeciwnym wypadku zmniejsz:
        # (W rzeczywistej implementacji trzeba by przechowywac poprzednia strate)

        if loss.item() > 0.6:
            meta_alpha *= 1.1
        else:
            meta_alpha *= 0.95

        for param_group in optimizer.param_groups:
            param_group['lr'] = init_lr * meta_alpha

        optimizer.step()

        with torch.no_grad():
            pred = torch.sigmoid(logits) > 0.5
            acc = (pred == y).float().mean().item()
        print(f"[Epoch {epoch}] Loss: {loss.item():.4f} | Acc: {acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

if __name__ == '__main__':
    dynamic_lr_training()
\end{lstlisting}

\paragraph{Komentarz:}
\begin{itemize}
    \item Kod jest prosty (jeden MLP).
    \item Dynamiczna adaptacja LR jest \emph{prymitywna}, ale pokazuje ideę, że można sterować parametrem na bieżąco na podstawie stanu treningu.
    \item „\emph{meta\_alpha}” można traktować jako pewien \textbf{meta-hiperparametr}, który można by dalej uczyć w sposób bardziej zaawansowany (np. w MAML lub RL).
\end{itemize}

\subsubsection{Przykład w Zig: prosta koncepcja HPC + adaptacja hiperparametru}

Poniżej kod w \textbf{Zig}, w którym:
\begin{enumerate}
    \item Definiujemy prostą funkcję \texttt{trainStep} do wykonania kroku SGD.
    \item Zakładamy, że mamy \texttt{Matrix} i podstawowe operacje (macierzowe mnożenie, itp.).
    \item Demonstrujemy koncepcję adaptacji LR w stylu meta-learningu (pseudo).
\end{enumerate}

\begin{lstlisting}[language=zig, caption=Prosty szkielet HPC w Zig z adaptacją LR]
const std = @import("std");

pub const FloatT = f64; // moze być f32, w zależności od potrzeb

pub const Matrix = struct {
    data: []FloatT,
    rows: usize,
    cols: usize,
};

pub fn main() !void {
    var allocator = std.heap.page_allocator;

    // Przykładowe parametry (np. wagi prostej sieci).
    // W realnych zastosowaniach mielibyśmy np. warstwy itp.
    var w0: FloatT = 0.1;
    var w1: FloatT = 0.2;
    var b: FloatT  = 0.0;

    // Sztuczne dane (mała próbka)
    const xData = [_]FloatT { -1.0, 0.5, 1.0, 2.0 };
    const yData = [_]FloatT { -2.0, 0.0, 1.5,  2.5 };

    var lr: FloatT = 0.01;
    var meta_factor: FloatT = 1.0;

    var old_loss: FloatT = 9999.0;

    for (0..100) |epoch| {
        // Policz gradient i stratę
        var loss: FloatT = 0.0;
        var grad_w0: FloatT = 0.0;
        var grad_w1: FloatT = 0.0;
        var grad_b:  FloatT = 0.0;

        for (0..xData.len) |i| {
            const pred = w0 * xData[i] + w1 * (xData[i] * xData[i]) + b;
            const err = pred - yData[i];
            loss += err * err;
            // gradienty
            grad_w0 += 2.0 * err * xData[i];
            grad_w1 += 2.0 * err * xData[i] * xData[i];
            grad_b  += 2.0 * err;
        }
        loss /= @as(FloatT, xData.len);

        // Pseudo meta-uczenie: dostosuj meta_factor
        if (loss > old_loss) {
            meta_factor *= 1.05; // rosnąco
        } else {
            meta_factor *= 0.95;
        }

        // Zaktualizuj parametry
        const effective_lr = lr * meta_factor;
        w0 -= effective_lr * grad_w0;
        w1 -= effective_lr * grad_w1;
        b  -= effective_lr * grad_b;

        // Debug
        std.debug.print(
            "Epoch={d}, Loss={f}, w0={f}, w1={f}, b={f}, LR={f}\n",
            .{ epoch, loss, w0, w1, b, effective_lr }
        );

        old_loss = loss;
    }
}
\end{lstlisting}

\paragraph{Omówienie:}
\begin{itemize}
    \item \texttt{meta\_factor} pełni tu funkcję prostego „kontrolera” intensywności uczenia (learning rate).  
    \item Gdy błąd (strata) wzrasta, zakładamy, że „uczymy się źle” i zwiększamy LR (być może to paradoksalne — można odwrotnie!).  
    \item Gdy błąd maleje, zmniejszamy LR, aby precyzyjniej dostrajać.  
    \item W rzeczywistości meta-learning jest dużo bardziej wyrafinowany: np. MAML, RL, czy gradient heurystyk hiperparametrów.
\end{itemize}

\section{Wyzwania praktyczne i dobre praktyki}

\subsection{Projektowanie zestawu zadań (meta-dataset)}

Aby meta-uczenie było skuteczne, potrzebny jest \textbf{zestaw zadań}, na których meta-learner będzie się uczył. W praktyce:
\begin{itemize}
    \item Zadania muszą być \textit{różnorodne} (np. różne dziedziny, rodzaje danych).
    \item Musi istnieć spójny sposób oceny postępów (np. ewaluacja na zbiorach walidacyjnych).
    \item Unikamy „przekłucia” (overfittingu) do jednego zadania — meta-uczeń powinien generalizować do \textbf{nowych zadań}.
\end{itemize}

\subsection{Koszty obliczeniowe i HPC}

Meta-learning potrafi być \textbf{kosztowny} obliczeniowo, ponieważ wielokrotnie trenujemy i oceniamy model na różnych zadaniach. Dlatego ważne są:
\begin{itemize}
    \item Wydajne implementacje obliczeń (np. GPU, SIMD na CPU).
    \item \emph{Przemyślana} inżynieria datasetów (np. batch tasks, mini-batching).
    \item Techniki kwantyzacji (4-bit, 8-bit) lub niskorangowej adaptacji (LoRA), aby ograniczyć rozmiar modelu.
\end{itemize}

Z tego powodu język taki jak \textbf{Zig} (z niskopoziomowym dostępem do zasobów) może być dobrym wyborem przy implementacji wydajnych fragmentów kodu.

\section{Podsumowanie}

\textbf{Meta-learning} stanowi perspektywiczny kierunek w uczeniu maszynowym, pozwalający \emph{uczyć się} jak optymalizować parametry i strukturę sieci neuronowych w różnych zadaniach. Dzięki temu:
\begin{itemize}
    \item Możemy automatycznie dostrajać hiperparametry (learning rate, momentum, itp.).
    \item Usprawniamy \emph{few-shot learning} (szybkie adaptacje do nowych problemów).
    \item Odciążamy człowieka od żmudnych poszukiwań w przestrzeni hiperparametrów.
\end{itemize}

Współczesne wdrożenia często łączą \emph{meta-learning} z dużymi modelami (np. GPT, Whisper) i metodami HPC (CUDA, wielowątkowość), aby efektywnie przetwarzać duże wolumeny danych. \textbf{Zig} może pełnić ważną rolę w tych zastosowaniach, zapewniając wydajność i elastyczność przy tworzeniu niskopoziomowych bibliotek do obliczeń macierzowych, implementacji gradientów czy zarządzania pamięcią.

\subsection*{Możliwe kierunki dalszych badań}
\addcontentsline{toc}{subsection}{Możliwe kierunki dalszych badań}

\begin{itemize}
    \item \textbf{Połączenie meta-learningu z metodami ewolucyjnymi}: automatyczna ewolucja architektury i hiperparametrów.
    \item \textbf{Zastosowanie w robotyce i systemach sterowania}: meta-learning do adaptacji kontrolerów w środowiskach zmiennych.
    \item \textbf{Rozszerzenie MAML na scenariusze multi-modalne} (np. audio+tekst+obraz).
    \item \textbf{Badania interpretowalności}: jak zrozumieć wewnętrzne mechanizmy meta-uczenia.
\end{itemize}

\section*{Bibliografia}
\addcontentsline{toc}{section}{Bibliografia}

\begin{thebibliography}{9}

\bibitem[Hochreiter et al.(2001)]{hochreiter2001learning}
S.~Hochreiter, A.~Younger, and P.~Conwell.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks (ICANN)}, 2001.

\bibitem[Finn et al.(2017)]{finn2017maml}
C.~Finn, P.~Abbeel, S.~Levine.
\newblock Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
\newblock \emph{ICML}, 2017.

\bibitem[Ravi \& Larochelle(2017)]{ravi2016optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a Model for Few-Shot Learning.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}

\end{document}
